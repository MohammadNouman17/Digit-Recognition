{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "first-order-model-demo",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammadNouman17/Digit-Recognition/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdO_RxQZLahB"
      },
      "source": [
        "# Demo for paper \"First Order Motion Model for Image Animation\"\n",
        "To try the demo, press the 2 play buttons in order and scroll to the bottom. Note that it may take several minutes to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCMFMJV7K-ag"
      },
      "source": [
        "%%capture\n",
        "# Install required dependencies\n",
        "%pip install ffmpeg-python imageio-ffmpeg\n",
        "\n",
        "# Initialize Git repository and clone the new model repository\n",
        "!git init .\n",
        "!git remote add origin https://github.com/graphemecluster/custom-first-order-model\n",
        "!git pull origin main\n",
        "!git clone https://github.com/graphemecluster/custom-first-order-model demo\n",
        "\n",
        "# Clone new and diverse datasets\n",
        "\n",
        "# 1. VoxCeleb2 dataset (video and audio for speaker recognition)\n",
        "!git clone https://github.com/robots-oxford/voxceleb_voxceleb2.git /content/voxceleb2\n",
        "\n",
        "# 2. UCF101 (human action recognition videos)\n",
        "!wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101.rar -O /content/UCF101.rar\n",
        "!unrar x /content/UCF101.rar /content/UCF101\n",
        "\n",
        "# 3. Open Images Dataset (large-scale diverse image dataset)\n",
        "!git clone https://github.com/cvdfoundation/open-images-dataset.git /content/open-images\n",
        "\n",
        "# 4. Kinetics-700 (videos for human activity understanding)\n",
        "!wget --no-check-certificate https://storage.googleapis.com/deepmind-media/Datasets/kinetics700.tar.gz -O /content/kinetics700.tar.gz\n",
        "!tar -xvzf /content/kinetics700.tar.gz -C /content/kinetics700\n",
        "\n",
        "# 5. CelebV-HQ dataset (high-quality videos for face modeling)\n",
        "!wget --no-check-certificate https://github.com/CelebV-HQ/celebv-hq/archive/refs/heads/main.zip -O /content/CelebV-HQ.zip\n",
        "!unzip /content/CelebV-HQ.zip -d /content/celebv-hq\n",
        "\n",
        "# 6. Sports-1M Dataset (sports-related video dataset)\n",
        "!wget --no-check-certificate http://data.yt8m.org/download.py -O /content/sports-1m.py\n",
        "!python3 /content/sports-1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxi6-riLOgnm"
      },
      "source": [
        "# Update model list based on the links provided\n",
        "model = ipywidgets.Dropdown(\n",
        "    description=\"Model:\",\n",
        "    options=[\n",
        "        'vox',\n",
        "        'vox-adv',\n",
        "        'taichi',\n",
        "        'taichi-adv',\n",
        "        'nemo',\n",
        "        'mgif',\n",
        "        'fashion',\n",
        "        'bair'\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Custom behavior based on model selection\n",
        "def change_model(change):\n",
        "    if model.value.startswith('vox'):\n",
        "        warning.remove_class('warn')  # Show or hide warning\n",
        "    else:\n",
        "        warning.add_class('warn')  # Show or hide warning\n",
        "model.observe(change_model, names='value')\n",
        "\n",
        "# Image upload logic\n",
        "def upload_image(change):\n",
        "    global selected_image\n",
        "    for name, file_info in upload_input_image_button.value.items():\n",
        "        content = file_info['content']\n",
        "    if content is not None:\n",
        "        selected_image = resize(PIL.Image.open(io.BytesIO(content)).convert(\"RGB\"))\n",
        "        input_image_widget.clear_output(wait=True)\n",
        "        with input_image_widget:\n",
        "            display(selected_image)\n",
        "        input_image_widget.add_class('uploaded')\n",
        "        display(Javascript('deselectImages()'))\n",
        "\n",
        "upload_input_image_button.observe(upload_image, names='value')\n",
        "\n",
        "# Video upload logic\n",
        "def upload_video(change):\n",
        "    global selected_video\n",
        "    for name, file_info in upload_input_video_button.value.items():\n",
        "        content = file_info['content']\n",
        "    if content is not None:\n",
        "        selected_video = 'user/' + name\n",
        "        with open(selected_video, 'wb') as video:\n",
        "            video.write(content)\n",
        "        preview = resize(PIL.Image.fromarray(thumbnail(selected_video)).convert(\"RGB\"))\n",
        "        input_video_widget.clear_output(wait=True)\n",
        "        with input_video_widget:\n",
        "            display(preview)\n",
        "        input_video_widget.add_class('uploaded')\n",
        "        display(Javascript('deselectVideos()'))\n",
        "\n",
        "upload_input_video_button.observe(upload_video, names='value')\n",
        "\n",
        "\n",
        "def generate(button):\n",
        "    main.layout.display = 'none'\n",
        "    loading.layout.display = ''\n",
        "\n",
        "    # Ensure correct checkpoint file is loaded based on the selected model\n",
        "    filename = model.value + ('' if model.value == 'fashion' else '-cpk') + '.pth.tar'\n",
        "    if not os.path.isfile(filename):\n",
        "        response = requests.get('https://github.com/graphemecluster/first-order-model-demo/releases/download/checkpoints/' + filename, stream=True)\n",
        "        with progress_bar:\n",
        "            with tqdm.wrapattr(response.raw, 'read', total=int(response.headers.get('Content-Length', 0)), unit='B', unit_scale=True, unit_divisor=1024) as raw:\n",
        "                with open(filename, 'wb') as file:\n",
        "                    copyfileobj(raw, file)\n",
        "        progress_bar.clear_output()\n",
        "\n",
        "    reader = imageio.get_reader(selected_video, mode='I', format='FFMPEG')\n",
        "    fps = reader.get_meta_data()['fps']\n",
        "    driving_video = [frame for frame in reader]\n",
        "\n",
        "    # Load the selected model checkpoint\n",
        "    generator, kp_detector = load_checkpoints(config_path='config/%s-256.yaml' % model.value, checkpoint_path=filename)\n",
        "\n",
        "    # Generate the animation with the selected video and image\n",
        "    with progress_bar:\n",
        "        predictions = make_animation(\n",
        "            skimage.transform.resize(numpy.asarray(selected_image), (256, 256)),\n",
        "            [skimage.transform.resize(frame, (256, 256)) for frame in driving_video],\n",
        "            generator,\n",
        "            kp_detector,\n",
        "            relative=relative.value,\n",
        "            adapt_movement_scale=adapt_movement_scale.value\n",
        "        )\n",
        "    progress_bar.clear_output()\n",
        "\n",
        "    # Save the generated video as output\n",
        "    imageio.mimsave('output.mp4', [img_as_ubyte(frame) for frame in predictions], fps=fps)\n",
        "\n",
        "    # Combine the video with audio\n",
        "    try:\n",
        "        with NamedTemporaryFile(suffix='.mp4') as output:\n",
        "            ffmpeg.output(ffmpeg.input('output.mp4').video, ffmpeg.input(selected_video).audio, output.name, c='copy').run()\n",
        "            with open('output.mp4', 'wb') as result:\n",
        "                copyfileobj(output, result)\n",
        "    except ffmpeg.Error:\n",
        "        pass\n",
        "\n",
        "    # Display the output video\n",
        "    output_widget.clear_output(True)\n",
        "    with output_widget:\n",
        "        video_widget = ipywidgets.Video.from_file('output.mp4', autoplay=False, loop=False)\n",
        "        video_widget.add_class('video')\n",
        "        display(video_widget)\n",
        "\n",
        "    # Display comparison video (original vs generated)\n",
        "    comparison_widget.clear_output(True)\n",
        "    with comparison_widget:\n",
        "        video_widget = ipywidgets.Video.from_file(selected_video, autoplay=False, loop=False, controls=False)\n",
        "        video_widget.add_class('video')\n",
        "        display(video_widget)\n",
        "\n",
        "    # Synchronize the playback between original and generated videos\n",
        "    display(Javascript(\"\"\"\n",
        "    setTimeout(function() {\n",
        "        (function(left, right) {\n",
        "            left.addEventListener(\"play\", function() { right.play(); });\n",
        "            left.addEventListener(\"pause\", function() { right.pause(); });\n",
        "            left.addEventListener(\"seeking\", function() { right.currentTime = left.currentTime; });\n",
        "            right.muted = true;\n",
        "        })(document.getElementsByClassName(\"video-left\")[0], document.getElementsByClassName(\"video-right\")[0]);\n",
        "    }, 1000);\n",
        "    \"\"\"))\n",
        "\n",
        "    # Finalize UI update\n",
        "    loading.layout.display = 'none'\n",
        "    complete.layout.display = ''\n",
        "\n",
        "def resize(image, size=(256, 256)):\n",
        "    w, h = image.size\n",
        "    d = min(w, h)\n",
        "    r = ((w - d) // 2, (h - d) // 2, (w + d) // 2, (h + d) // 2)\n",
        "    return image.resize(size, resample=PIL.Image.LANCZOS, box=r)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPNFqtXDKpTP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}